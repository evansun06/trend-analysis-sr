{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d85c80",
   "metadata": {},
   "source": [
    "## The Impact of Intermediate Dimensionality on the Clustering Coherence of News Embeddings using HDBSCAN\n",
    "---\n",
    "This notebook evaluates clustering pipelines for news article embeddings by systematically varying intermediate dimensionality reduction and measuring cluster coherence, coverage, and stability. We show that clustering quality is highly sensitive to the reduced dimension and that stable, interpretable trends emerge only within a narrow range of pipeline configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2b0ff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7e8cb1",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "---\n",
    "Recent progress in transformer-based language models has made it straightforward to represent large collections of text as high-dimensional semantic embeddings. Because these embeddings are often massive (over 3,000 dimensions), we rarely cluster them directly. Instead, we use \"intermediate\" steps to shrink the data down before grouping it.\n",
    "\n",
    "Why do we do this? Direct clustering in high dimension is hindered by _distance concentration_. Geometrically, as dimensionality increases, the contrast between the distance to the nearest and farthest neighbors diminishes. Density-based algorithms like [HDBSCAN](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html) identifies clusters by evaluating the distance and connectivity of a Minimum Spanning Tree. This 'flatness' in distance prevents the algorithm from distinguishing between dense topical cores and the sparse background noise. Intermediate dimensionality reduction serves to __restore the local density contrast__ necessary for HDBSCAN’s reachability metrics to function.\n",
    "\n",
    "In most data pipelines, developers use Principal Component Analysis (PCA) or Uniform Manifold Approximation and Projection (UMAP) to handle this shrinkage. While PCA is a classic, linear approach that focuses on broad patterns, UMAP is a modern, non-linear method designed to keep related items close together. However, after searching for a 'best' intermediate dimension, most sources pointed to heuristics.\n",
    "\n",
    "This choice is especially important for Hierarchical Density-Based Spatial Clustering of Applications with Noise(HDBSCAN), a clustering algorithm that groups data based on how \"dense\" it is. Because the very concept of \"density\" changes depending on how many dimensions you have, the intermediate step isn't just a technicality—it might completely change which news stories get grouped together and which are thrown out as \"noise.\"\n",
    "\n",
    "### The Goal\n",
    "---\n",
    "I'm particularly intrested in using HDBSCAN for automating trend detection in news for a digestive reading app called SightRead. Through this project I hope to learn more about the tradeoffs in choosing a clustering pipeline, and simply document findings. \n",
    "\n",
    "In this work, I explore how changing the number of intermediate dimensions for both PCA and UMAP affects the final quality of news clusters. By testing a range of dimensions, I aim to measure:\n",
    "\n",
    "__Cluster Coherence__: How semantically similar are the articles within a single group?\n",
    "\n",
    "__Noise Levels__: How much of our data is discarded as \"un-clusterable\" in different dimensions?\n",
    "\n",
    "__Method Comparison__: Does UMAP’s non-linear approach actually produce better clusters for news than the simpler PCA method?\n",
    "\n",
    "By analyzing embeddings from RSS news feeds, I hope to provide a practical guide for which dimensionality reduction settings actually work best for organizing the daily news."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d91021",
   "metadata": {},
   "source": [
    "### An Overview of Data Collection\n",
    "---\n",
    "I'll give a quick overview of where and how the data is collected, more information on the dataset can be found [here](https://github.com/evansun06/trend-analysis-sr/blob/main/wrangling.ipynb). Articles were polled for a 1-week window from January 5th to January 12th from a select subset of RSS feeds from _The Guardian_, _The Wall Street Journal_, _United Nations_, _Fox News_, and _Daily Mail_.\n",
    "\n",
    "<p align=\"center\">\n",
    "\t<img src=\"assets/rss_aggregation.png\" alt=\"Data aggregation diagram\" width=\"720\" />\n",
    "</p>\n",
    "\n",
    "_figure: SightRead RSS aggregation workflow_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69990b0",
   "metadata": {},
   "source": [
    "### Methodology Part I: Effect of Intermediate Dimensionality on the Cluster Quality of HDBSCAN with respect to PCA and UMAP\n",
    "---\n",
    "\n",
    "__Pipeline configurations__ $(d, {mcs}, {ms})$:\n",
    "- intermediate dimension $d \\in \\{2, 5, 10, 15, 25, 50, 75, 100\\}$, we run the following pipelines.\n",
    "- min_cluster_size(mcs): HDBSCAN hyperparameter $\\{5, 10, 15, 30\\}$\n",
    "- min_samples(ms): HDBSCAN hyperparameter $\\{5, 10, 15\\}$\n",
    "\n",
    "__Data and Embeddings__\n",
    "\n",
    "Let $N$ be the number of news articles contained in the dataset. Each article $i$ is represented by an embedding vector\n",
    "\n",
    "$$\n",
    "x_i \\in \\mathbb{R}^D\n",
    "$$\n",
    "\n",
    "where D is the embedding dimension (3072 in our case).\n",
    "\n",
    "We stack embeddings into a matrix $X \\in \\mathbb{R}^{D\\times N}$ where\n",
    "\n",
    "$$\n",
    "\n",
    "X = \\begin{bmatrix} x_1^{\\top} \\\\ x_2^{\\top} \\\\ \\vdots \\\\ x_N^{\\top}\\end{bmatrix} \\in \\mathbb{R}^{N \\times D}\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327a7469",
   "metadata": {},
   "source": [
    "For each pipeline configuration we run both clustering pipelines:\n",
    "\n",
    "__A. Principle Component Analysis__\n",
    "\n",
    "1. L2 Normalization (scikit-learn)\n",
    "$$\n",
    "\\forall x_i \\in X \\rightarrow \\hat{x}_i = \\frac{x_i}{||x_i||_2}\n",
    "$$\n",
    "2. Reduce to intermediate dimension via PCA\n",
    "$$\n",
    "    {PCA}_{(d)}: \\mathbb{R}^D \\rightarrow \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "$$\n",
    "    X \\rightarrow Z^{(pca)}_d,\\ \\ \\  Z^{(pca)}_d\\in \\mathbb{R}^{N \\times d}\n",
    "$$\n",
    "\n",
    "3. Cluster using HDBSCAN with variated hyperparameters\n",
    "    - `min_cluster_size`\n",
    "    - `min_samples`\n",
    "    - `output`: Assigns each article $i$ with label $\\ell_i \\in \\{-1, 0, 1, ..., K-1\\}$ where -1 is noise\n",
    "\n",
    "__B. Unifold Manifold Approximation and Projection__\n",
    "- Repeat with UMAP\n",
    "$$\n",
    "{UMAP}_{(d)}: \\mathbb{R}^D \\rightarrow \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "$$\n",
    "    X \\rightarrow Z^{(umap)}_d,\\ \\ \\  Z^{(umap)}_d\\in \\mathbb{R}^{N \\times d}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eef955",
   "metadata": {},
   "source": [
    "### Metrics \n",
    "After performing clustering for all tuples(d, mcs, ms) for both piplines we select the optimized pipeline configuration for both UMAP and PCA via these metrics below.\n",
    "\n",
    "Let $K$ denote the number of clusters discovered by HDBSCAN for a given run, excluding noise. We index clusters by $k \\in \\{0, 1, ..., K - 1\\}$, with noise labeled as $-1$.\n",
    "\n",
    "\n",
    "__1.  Cluster coherence__\n",
    "\n",
    "Cluster to centroid cosine similarity. Represents the semantic coherence of a cluster\n",
    "\n",
    "For each cluster $k \\in \\{0, ..., K-1\\}$, let $S_k$ be the set of all articles $i$ grouped within cluster $k$, and $n_k = |S_k|$\n",
    "\n",
    "__1.1__ Compute the centroid (mean embedding):\n",
    "\n",
    "$$\n",
    "    \\mu_k = \\frac{1}{n_k}\\sum_{i \\in S_k}{\\hat{x}_i}\n",
    "\n",
    "$$\n",
    "\n",
    "__1.2__ Normalize the centroid\n",
    "\n",
    "$$\n",
    "    \\hat{\\mu}_k = \\frac{\\mu_k}{||\\mu_k||_2}\n",
    "\n",
    "$$\n",
    "\n",
    "__1.3__ Cluster coherence score\n",
    "\n",
    "For each article $i$ in the cluster set, $S_k$, we calculate the cosine simlarity to the centroid $\\hat{\\mu}_k$, and average.\n",
    "\n",
    "$$\n",
    "    \\text{coh}(k) = \\frac{1}{n_k} \\sum_{i\\in S_k} \\hat{x}_i^\\top \\hat{\\mu}_k\n",
    "$$\n",
    "\n",
    "__1.4__ Size-weighted overall coherence\n",
    "\n",
    "Primary metric for pipeline cluster coherence, weighted to counteract disproportional clusters.\n",
    "\n",
    "$$\n",
    "    \\text{coh}_{weighted} = \\frac{\\sum^{K-1}_{k=0}{n_k \\text{coh}(k)}}{\\sum^{K-1}_{k=0}{n_k}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "__2. Cluster and Noise Fraction__\n",
    "\n",
    "Let the set of clustered points be \n",
    "\n",
    "$$\n",
    "\\mathcal{C} = \\{i | \\ell_i \\neq -1 \\}, \\ \\ |\\mathcal{C}| = N_c\n",
    "$$\n",
    "\n",
    "let the set of noise points be \n",
    "\n",
    "$$\n",
    "\\mathcal{N} = \\{i | \\ell_i = -1\\},\\ \\  |\\mathcal{N}| = N_n\n",
    "$$\n",
    "\n",
    "and $N = N_c + N_n$.\n",
    "\n",
    "Then we define the Noise Fraction as `noise_frac` = $\\frac{N_n}{N}$ and Cluster Fraction as `cluster_frac` = $\\frac{N_c}{N}$\n",
    "\n",
    "\n",
    "__3. Sanity Metrics__\n",
    "\n",
    "Avoids trivial cases\n",
    "1.  Number of clusters: $K$\n",
    "2.  Largest cluster fraction:\n",
    "$$\n",
    "\\frac{max_k n_k}{N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035cfe4d",
   "metadata": {},
   "source": [
    "### Methodology Part II: Comparative Pipeline Evaluation and Stability Analysis\n",
    "---\n",
    "\n",
    "__1. Cross-Pipeline Evaluation__\n",
    "\n",
    "To determine the superior dimensionality reduction strategy, we compare the optimized PCA and UMAP pipelines. This comparison weighs the trade-off between internal semantic density and topological robustness.\n",
    "    - __Relative Coherence Gain__: The percentage difference in $\\text{coh}_{weighted}$ between PCA and UMAP.\n",
    "    - __Granularity__: A comparison of $K_{pca}$ versus $K_{umap}$ to identify which method captures a more nuanced topic hierarchy.\n",
    "    - __Case-by-case observations__: View select clusters by hand and document observervations.\n",
    "\n",
    "__2. Stability Analysis via Subsampling__\n",
    "\n",
    "To ensure the discovered clusters represent robust semantic structures rather than stochastic one-offs, we perform a stability stress test.\n",
    "\n",
    "__2.1__ Subsampling Procedure\n",
    "\n",
    "We perform $M$ iterations of subsampling without replacement.\n",
    "\n",
    "- In each iteration $m$, a subset $I_m \\subset I$ is drawn such that $|I_m| = 0.8N$.\n",
    "- We then run the optimized PCA and UMAP pipelines for each iteration.\n",
    "\n",
    "*_Subsampling without replacement is critical to prevent \"artificial coherence\" caused by identical vector duplicates, which would otherwise bias the cosine similarity scores._\n",
    "\n",
    "__2.2__ Consistency Metrics\n",
    "\n",
    "For each iteration $m$, we re-calculate and track metrics like _relative coherence gain_, $K$, $\\text{coh}_{weighted}$. After all M iterations we observe:\n",
    "    \n",
    "- __Cluster Count Variance__ ($\\sigma^2_K$): Measures the decisiveness of the pipeline.\n",
    "$$\n",
    "\\sigma^2_K = \\frac{1}{M-1} \\sum_{m=1}^{M} (K_m - \\bar{K})^2\n",
    "$$\n",
    "\n",
    "- __Coherence Stability__: The standard deviation of $\\text{coh}_{weighted}$ across runs, indicating if the \"thematic tightness\" is dependent on specific articles.\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\sigma_{\\text{coh}_{weighted}} = \\sqrt{\\frac{1}{M-1} \\sum_{m=1}^{M} ({\\text{coh}_{weighted}}_m - {\\text{coh}_{weighted}}_{avg})^2}\n",
    "$$\n",
    "\n",
    "- __Adjusted Rand Index (ARI)__: The primary measure of __Label Persistence__. For the set of points present in both the original run (labels $L_{orig}$) and the subsampled run (labels $L_m$), we calculate:\n",
    "\n",
    "$$ARI(L_{orig}, L_m) = \\frac{\\sum_{ij} \\binom{n_{ij}}{2} - [\\sum_i \\binom{a_i}{2} \\sum_j \\binom{b_j}{2}] / \\binom{n}{2}}{\\frac{1}{2} [\\sum_i \\binom{a_i}{2} + \\sum_j \\binom{b_j}{2}] - [\\sum_i \\binom{a_i}{2} \\sum_j \\binom{b_j}{2}] / \\binom{n}{2}}$$\n",
    "- An $ARI \\approx 1.0$ indicates perfect cluster survival.\n",
    "- An $ARI \\approx 0$ indicates that the clusters are essentially random and do not persist across data perturbations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb1b6e7",
   "metadata": {},
   "source": [
    "### Dataset overview\n",
    "\n",
    "__Fields__ (post-wrangling):\n",
    "1. article_id: UUID for a unique article.\n",
    "2. ingestion_id: UUID for the ingested article (articles can be ingested in different ways, in this dataset all used the `default` version).\n",
    "3. normalized_text: Headline concatenated with description, normalized.\n",
    "4. polled_at: timestamp when the article was polled at (each RSS feed was polled by the hour)\n",
    "5. published_at: the publication date of the article for possible time series analysis\n",
    "6. embedding: the embedding vector (OpenAI text-embedding-3-large)\n",
    "\n",
    "*_see `wrangling.ipynb` for wrangling procedures_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e72a11a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>ingestion_id</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>polled_at</th>\n",
       "      <th>published_at</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d9509dc9-a405-4866-9201-19e3d063d137</td>\n",
       "      <td>000d8dea-8f60-470e-b539-eb51780b55db</td>\n",
       "      <td>Outrage as 'miserable' shovel-wielding neighbo...</td>\n",
       "      <td>2026-01-07 13:33:24.714064+00</td>\n",
       "      <td>2026-01-07 13:29:24+00</td>\n",
       "      <td>[-0.01143522 -0.03746951 -0.00753336 ... -0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9a156ce2-11d6-4eff-b243-e2d2bb208faa</td>\n",
       "      <td>0072ab60-b1ed-42a9-9624-88c18292e588</td>\n",
       "      <td>Morality, military might and a sense of mischi...</td>\n",
       "      <td>2026-01-09 05:17:56.535446+00</td>\n",
       "      <td>2026-01-09 03:15:08+00</td>\n",
       "      <td>[ 0.00615122 -0.03799464 -0.01626345 ...&nbsp;&nbsp;0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>df3b2fc9-4d0f-4199-bca1-ea6d954e0c31</td>\n",
       "      <td>007f72ee-5039-47bb-8c61-c981f2810d3a</td>\n",
       "      <td>Republican senator vows to block all Fed nomin...</td>\n",
       "      <td>2026-01-12 15:17:02.967673+00</td>\n",
       "      <td>2026-01-12 14:55:53+00</td>\n",
       "      <td>[-0.00727808 -0.02314259 -0.02138059 ...&nbsp;&nbsp;0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d0f6077e-15be-497e-9a2b-1c023486ce9a</td>\n",
       "      <td>00879af4-8278-40a8-8338-dba8a0b45810</td>\n",
       "      <td>Starmer prepares to rip up Brexit: PM ready to...</td>\n",
       "      <td>2026-01-05 04:42:39.19293+00</td>\n",
       "      <td>2026-01-04 15:49:23+00</td>\n",
       "      <td>[ 0.01808163&nbsp;&nbsp;0.02099675 -0.01054433 ...&nbsp;&nbsp;0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>847a6cef-0c95-4a0f-9388-7345ba01bf26</td>\n",
       "      <td>00937788-6ba8-4db8-8107-645754dd7362</td>\n",
       "      <td>ANOTHER poll shows Labour in third behind Refo...</td>\n",
       "      <td>2026-01-07 11:12:56.5161+00</td>\n",
       "      <td>2026-01-07 10:57:35+00</td>\n",
       "      <td>[ 0.03266984 -0.00281101 -0.01202355 ...&nbsp;&nbsp;0.01...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             article_id                          ingestion_id  \\\n",
       "0  d9509dc9-a405-4866-9201-19e3d063d137  000d8dea-8f60-470e-b539-eb51780b55db   \n",
       "1  9a156ce2-11d6-4eff-b243-e2d2bb208faa  0072ab60-b1ed-42a9-9624-88c18292e588   \n",
       "2  df3b2fc9-4d0f-4199-bca1-ea6d954e0c31  007f72ee-5039-47bb-8c61-c981f2810d3a   \n",
       "3  d0f6077e-15be-497e-9a2b-1c023486ce9a  00879af4-8278-40a8-8338-dba8a0b45810   \n",
       "4  847a6cef-0c95-4a0f-9388-7345ba01bf26  00937788-6ba8-4db8-8107-645754dd7362   \n",
       "\n",
       "                                     normalized_text  \\\n",
       "0  Outrage as 'miserable' shovel-wielding neighbo...   \n",
       "1  Morality, military might and a sense of mischi...   \n",
       "2  Republican senator vows to block all Fed nomin...   \n",
       "3  Starmer prepares to rip up Brexit: PM ready to...   \n",
       "4  ANOTHER poll shows Labour in third behind Refo...   \n",
       "\n",
       "                       polled_at            published_at  \\\n",
       "0  2026-01-07 13:33:24.714064+00  2026-01-07 13:29:24+00   \n",
       "1  2026-01-09 05:17:56.535446+00  2026-01-09 03:15:08+00   \n",
       "2  2026-01-12 15:17:02.967673+00  2026-01-12 14:55:53+00   \n",
       "3   2026-01-05 04:42:39.19293+00  2026-01-04 15:49:23+00   \n",
       "4    2026-01-07 11:12:56.5161+00  2026-01-07 10:57:35+00   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.01143522 -0.03746951 -0.00753336 ... -0.00...  \n",
       "1  [ 0.00615122 -0.03799464 -0.01626345 ...  0.00...  \n",
       "2  [-0.00727808 -0.02314259 -0.02138059 ...  0.01...  \n",
       "3  [ 0.01808163  0.02099675 -0.01054433 ...  0.00...  \n",
       "4  [ 0.03266984 -0.00281101 -0.01202355 ...  0.01...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/wrangled_articles.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fb8639",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220260e1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
