{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d85c80",
   "metadata": {},
   "source": [
    "## The Impact of Intermediate Dimensionality on the Clustering Coherence of News Embeddings using HDBSCAN\n",
    "---\n",
    "This notebook evaluates clustering pipelines for news article embeddings by systematically varying intermediate dimensionality reduction and measuring cluster coherence, coverage, and stability. We show that clustering quality is highly sensitive to the reduced dimension and that stable, interpretable trends emerge only within a narrow range of pipeline configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b0ff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Standard Library\n",
    "import uuid\n",
    "import ast\n",
    "\n",
    "# 2. Data Handling & Science\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 3. Dimensionality & Clustering\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import hdbscan\n",
    "\n",
    "# 4. Metrics\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "seed:int = 67"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7e8cb1",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "---\n",
    "Recent progress in transformer-based language models has made it straightforward to represent large collections of text as high-dimensional semantic embeddings. Because these embeddings are often massive (over 3,000 dimensions), we rarely cluster them directly. Instead, we use \"intermediate\" steps to shrink the data down before grouping it.\n",
    "\n",
    "Why do we do this? Direct clustering in high dimension is hindered by _distance concentration_. Geometrically, as dimensionality increases, the contrast between the distance to the nearest and farthest neighbors diminishes. Density-based algorithms like [HDBSCAN](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html) identifies clusters by evaluating the distance and connectivity of a Minimum Spanning Tree. This 'flatness' in distance prevents the algorithm from distinguishing between dense topical cores and the sparse background noise. Intermediate dimensionality reduction serves to __restore the local density contrast__ necessary for HDBSCAN’s reachability metrics to function.\n",
    "\n",
    "In most data pipelines, developers use Principal Component Analysis (PCA) or Uniform Manifold Approximation and Projection (UMAP) to handle this shrinkage. While PCA is a classic, linear approach that focuses on broad patterns, UMAP is a modern, non-linear method designed to keep related items close together. However, after searching for a 'best' intermediate dimension, most sources pointed to heuristics.\n",
    "\n",
    "This choice is especially important for Hierarchical Density-Based Spatial Clustering of Applications with Noise(HDBSCAN), a clustering algorithm that groups data based on how \"dense\" it is. Because the very concept of \"density\" changes depending on how many dimensions you have, the intermediate step isn't just a technicality—it might completely change which news stories get grouped together and which are thrown out as \"noise.\"\n",
    "\n",
    "### The Goal\n",
    "---\n",
    "I'm particularly intrested in using HDBSCAN for automating trend detection in news for a digestive reading app called SightRead. Through this project I hope to learn more about the tradeoffs in choosing a clustering pipeline, and simply document findings. \n",
    "\n",
    "In this work, I explore how changing the number of intermediate dimensions for both PCA and UMAP affects the final quality of news clusters. By testing a range of dimensions, I aim to measure:\n",
    "\n",
    "__Cluster Coherence__: How semantically similar are the articles within a single group?\n",
    "\n",
    "__Noise Levels__: How much of our data is discarded as \"un-clusterable\" in different dimensions?\n",
    "\n",
    "__Method Comparison__: Does UMAP’s non-linear approach actually produce better clusters for news than the simpler PCA method?\n",
    "\n",
    "By analyzing embeddings from RSS news feeds, I hope to provide a practical guide for which dimensionality reduction settings actually work best for organizing the daily news."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d91021",
   "metadata": {},
   "source": [
    "### An Overview of Data Collection\n",
    "---\n",
    "I'll give a quick overview of where and how the data is collected, more information on the dataset can be found [here](https://github.com/evansun06/trend-analysis-sr/blob/main/wrangling.ipynb). Articles were polled for a 1-week window from January 5th to January 12th from a select subset of RSS feeds from _The Guardian_, _The Wall Street Journal_, _United Nations_, _Fox News_, and _Daily Mail_.\n",
    "\n",
    "<p align=\"center\">\n",
    "\t<img src=\"assets/rss_aggregation.png\" alt=\"Data aggregation diagram\" width=\"720\" />\n",
    "</p>\n",
    "\n",
    "_figure: SightRead RSS aggregation workflow_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb1b6e7",
   "metadata": {},
   "source": [
    "### Dataset overview\n",
    "\n",
    "__Fields__ (post-wrangling):\n",
    "1. article_id: UUID for a unique article.\n",
    "2. normalized_text: Headline concatenated with description, normalized.\n",
    "3. polled_at: timestamp when the article was polled at (each RSS feed was polled by the hour)\n",
    "4. published_at: the publication date of the article for possible time series analysis\n",
    "5. embedding: the embedding vector (OpenAI text-embedding-3-large)\n",
    "\n",
    "*_see `wrangling.ipynb` for wrangling procedures_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e72a11a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>polled_at</th>\n",
       "      <th>published_at</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d9509dc9-a405-4866-9201-19e3d063d137</td>\n",
       "      <td>Outrage as 'miserable' shovel-wielding neighbo...</td>\n",
       "      <td>2026-01-07 13:33:24.714064+00</td>\n",
       "      <td>2026-01-07 13:29:24+00</td>\n",
       "      <td>[-0.011435215,-0.03746951,-0.0075333593,-0.026...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9a156ce2-11d6-4eff-b243-e2d2bb208faa</td>\n",
       "      <td>Morality, military might and a sense of mischi...</td>\n",
       "      <td>2026-01-09 05:17:56.535446+00</td>\n",
       "      <td>2026-01-09 03:15:08+00</td>\n",
       "      <td>[0.0061512175,-0.03799464,-0.016263446,-0.0129...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>df3b2fc9-4d0f-4199-bca1-ea6d954e0c31</td>\n",
       "      <td>Republican senator vows to block all Fed nomin...</td>\n",
       "      <td>2026-01-12 15:17:02.967673+00</td>\n",
       "      <td>2026-01-12 14:55:53+00</td>\n",
       "      <td>[-0.0072780806,-0.023142586,-0.021380594,0.020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d0f6077e-15be-497e-9a2b-1c023486ce9a</td>\n",
       "      <td>Starmer prepares to rip up Brexit: PM ready to...</td>\n",
       "      <td>2026-01-05 04:42:39.19293+00</td>\n",
       "      <td>2026-01-04 15:49:23+00</td>\n",
       "      <td>[0.018081628,0.02099675,-0.010544334,0.0247128...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>847a6cef-0c95-4a0f-9388-7345ba01bf26</td>\n",
       "      <td>ANOTHER poll shows Labour in third behind Refo...</td>\n",
       "      <td>2026-01-07 11:12:56.5161+00</td>\n",
       "      <td>2026-01-07 10:57:35+00</td>\n",
       "      <td>[0.032669835,-0.002811013,-0.012023548,-0.0104...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             article_id  \\\n",
       "0  d9509dc9-a405-4866-9201-19e3d063d137   \n",
       "1  9a156ce2-11d6-4eff-b243-e2d2bb208faa   \n",
       "2  df3b2fc9-4d0f-4199-bca1-ea6d954e0c31   \n",
       "3  d0f6077e-15be-497e-9a2b-1c023486ce9a   \n",
       "4  847a6cef-0c95-4a0f-9388-7345ba01bf26   \n",
       "\n",
       "                                     normalized_text  \\\n",
       "0  Outrage as 'miserable' shovel-wielding neighbo...   \n",
       "1  Morality, military might and a sense of mischi...   \n",
       "2  Republican senator vows to block all Fed nomin...   \n",
       "3  Starmer prepares to rip up Brexit: PM ready to...   \n",
       "4  ANOTHER poll shows Labour in third behind Refo...   \n",
       "\n",
       "                       polled_at            published_at  \\\n",
       "0  2026-01-07 13:33:24.714064+00  2026-01-07 13:29:24+00   \n",
       "1  2026-01-09 05:17:56.535446+00  2026-01-09 03:15:08+00   \n",
       "2  2026-01-12 15:17:02.967673+00  2026-01-12 14:55:53+00   \n",
       "3   2026-01-05 04:42:39.19293+00  2026-01-04 15:49:23+00   \n",
       "4    2026-01-07 11:12:56.5161+00  2026-01-07 10:57:35+00   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.011435215,-0.03746951,-0.0075333593,-0.026...  \n",
       "1  [0.0061512175,-0.03799464,-0.016263446,-0.0129...  \n",
       "2  [-0.0072780806,-0.023142586,-0.021380594,0.020...  \n",
       "3  [0.018081628,0.02099675,-0.010544334,0.0247128...  \n",
       "4  [0.032669835,-0.002811013,-0.012023548,-0.0104...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/wrangled_articles.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1329b215",
   "metadata": {},
   "source": [
    "### Part I: The Effect of Intermediate Dimensionality on the Cluster Quality of HDBSCAN with respect to PCA and UMAP\n",
    "\n",
    "The goal: In this section we'll explore and compare the effect of clustering with HDBSCAN in an particular intermediate dimension, $d$, on cluster coherence. We'll observe the effect under two pipelines which use UMAP and PCA to reduce dimensionality, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29ff7d3",
   "metadata": {},
   "source": [
    "\n",
    "Let us first define our embedding data mathematically:\n",
    "\n",
    "Let $N$ be the number of news articles contained in the dataset. Each article $i$ is represented by an embedding vector where $D$ is the embedding space (3072).\n",
    "\n",
    "$$\n",
    "x_i \\in \\mathbb{R}^D\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "We stack embeddings into a matrix $X \\in \\mathbb{R}^{D\\times N}$ where\n",
    "\n",
    "$$\n",
    "\n",
    "X = \\begin{bmatrix} x_1^{\\top} \\\\ x_2^{\\top} \\\\ \\vdots \\\\ x_N^{\\top}\\end{bmatrix} \\in \\mathbb{R}^{N \\times D}\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a722365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack raw embeddings into D x N numpy matrix\n",
    "df['embedding'] = df['embedding'].apply(ast.literal_eval)\n",
    "X_raw = np.vstack(df['embedding'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97557853",
   "metadata": {},
   "source": [
    "Below we define the two clustering piplines\n",
    "\n",
    "__Pipeline A: Unifold Manifold Approximation with Projection__\n",
    "\n",
    "1. L2 Normalization (scikit-learn)\n",
    "\n",
    "$$\\hat{X} = \\{ \\hat{x}_1, \\hat{x}_2, \\dots, \\hat{x}_n \\}^\\top \\quad \\text{where} \\quad \\hat{x}_i = \\frac{x_i}{\\|x_i\\|_2}$$\n",
    "\n",
    "2. Reduce to intermediate dimension via PCA\n",
    "$$\n",
    "    {PCA}_{(d)}: \\mathbb{R}^D \\rightarrow \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\hat{X} \\rightarrow Z^{(pca)}_d,\\ \\ \\  Z^{(pca)}_d\\in \\mathbb{R}^{N \\times d}\n",
    "$$\n",
    "\n",
    "3. Cluster using HDBSCAN with variated hyperparameters\n",
    "    - `min_cluster_size`\n",
    "    - `min_samples`\n",
    "    - `output`: Assigns each article $i$ with label $\\ell_i \\in \\{-1, 0, 1, ..., K-1\\}$ where -1 is noise\n",
    "\n",
    "\n",
    "__Pipeline B: Principle Component Analysis__\n",
    "\n",
    "_Repeat with UMAP_\n",
    "$$\n",
    "{UMAP}_{(d)}: \\mathbb{R}^D \\rightarrow \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\hat{X} \\rightarrow Z^{(umap)}_d,\\ \\ \\  Z^{(umap)}_d\\in \\mathbb{R}^{N \\times d}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b08392",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pipeline Helper Implemntation\n",
    "\n",
    "def cluster_hdbscan(mcs: int, ms: int, Z):\n",
    "    \"\"\"\n",
    "    Use HDBSCAN to clusters the reduced embeddings\n",
    "\n",
    "    args: \n",
    "        - mcs: minimum cluster size\n",
    "        - ms: minimum samples\n",
    "        - Z: the reduced embeddings matrix\n",
    "    \n",
    "    returns:\n",
    "        - labels: {-1, 0, ..., K - 1}\n",
    "    \"\"\"\n",
    "\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size= mcs,\n",
    "        min_samples= ms,\n",
    "        metric=\"euclidean\"\n",
    "    )   \n",
    "\n",
    "    return clusterer.fit_predict(Z)\n",
    "\n",
    "\n",
    "def reduce_umap(d: int, X_hat):\n",
    "    \"\"\"\n",
    "    Reduce embedding matrix to desired intermediate space via UMAP\n",
    "\n",
    "    args: \n",
    "        - d: intermediate dimension\n",
    "        - X_hat: normalized embeddings in original space\n",
    "\n",
    "    returns:\n",
    "        - Z_umap: matrix containing the reduced embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    # UMAP config for clustering (Heuristic Heavy)\n",
    "    reducer = umap.UMAP(\n",
    "        n_components=d, \n",
    "        n_neighbors=30,\n",
    "        min_dist=0.0, \n",
    "        metric=\"cosine\", \n",
    "        random_state=seed,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    return reducer.fit_transform(X_hat)\n",
    "    \n",
    "\n",
    "def reduce_pca(d: int, X_hat):\n",
    "    \"\"\"\n",
    "    Reduce embedding matrix to desired intermediate space via PCA\n",
    "\n",
    "    args: \n",
    "        - d: intermediate dimension\n",
    "        - X_hat: normalized embeddings in original space\n",
    "\n",
    "    returns:\n",
    "        - Z_pca: matrix containing the reduced embeddings\n",
    "    \"\"\"\n",
    "    \n",
    "    reducer = PCA(\n",
    "        n_components=d,\n",
    "        random_state=seed\n",
    "    )\n",
    "\n",
    "    return reducer.fit_transform(X_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8d2a00",
   "metadata": {},
   "source": [
    "### Variables\n",
    "\n",
    "__Pipeline configurations__ $(d, {mcs}, {ms})$:\n",
    "- intermediate dimension $d \\in \\{2, 3, 5, 10, 15, 25, 35, 50, 75, 100, 500\\}$\n",
    "    - covers a range of interesting dimensions including heuristic sweetspots (10-50) and pitfall ranges (100+)\n",
    "- min_cluster_size(mcs): HDBSCAN hyperparameter $\\{5, 10, 25, 50\\}$\n",
    "    - minimum size HDBSCAN will consider a legitimate, independent cluster rather than noise\n",
    "- min_samples(ms): HDBSCAN hyperparameter $\\{1, 5, 15, 30\\}$\n",
    "    - defines how many neighbors a point must have to be considered a \"core\" part of a dense region\n",
    "\n",
    "We'll compare a grid of min_cluster_size and min_samples as these hyperparameters are highly effected by dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061960b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 Normalize\n",
    "X_norm = Normalizer(norm='l2').fit_transform(X_raw)\n",
    "\n",
    "# Append original dataset for later reference\n",
    "df['embedding_norm'] = list(X_norm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb2ce40",
   "metadata": {},
   "source": [
    "### Quality Metrics \n",
    "After performing clustering for all tuples(d, mcs, ms) for both piplines we select the optimized pipeline configuration for both UMAP and PCA via these metrics below.\n",
    "\n",
    "Let $K$ denote the number of clusters discovered by HDBSCAN for a given run, excluding noise. We index clusters by $k \\in \\{0, 1, ..., K - 1\\}$, with noise labeled as $-1$.\n",
    "\n",
    "\n",
    "__1.  Cluster coherence__\n",
    "\n",
    "Cluster to centroid cosine similarity. Represents the semantic coherence of a cluster\n",
    "\n",
    "For each cluster $k \\in \\{0, ..., K-1\\}$, let $S_k$ be the set of all articles $i$ grouped within cluster $k$, and $n_k = |S_k|$\n",
    "\n",
    "__1.1__ Compute the centroid (mean embedding):\n",
    "\n",
    "$$\n",
    "    \\mu_k = \\frac{1}{n_k}\\sum_{i \\in S_k}{\\hat{x}_i}\n",
    "\n",
    "$$\n",
    "\n",
    "__1.2__ Normalize the centroid\n",
    "\n",
    "$$\n",
    "    \\hat{\\mu}_k = \\frac{\\mu_k}{||\\mu_k||_2}\n",
    "\n",
    "$$\n",
    "\n",
    "__1.3__ Cluster coherence score\n",
    "\n",
    "For each article $i$ in the cluster set, $S_k$, we calculate the cosine simlarity to the centroid $\\hat{\\mu}_k$, and average.\n",
    "\n",
    "$$\n",
    "    \\text{coh}(k) = \\frac{1}{n_k} \\sum_{i\\in S_k} \\hat{x}_i^\\top \\hat{\\mu}_k\n",
    "$$\n",
    "\n",
    "__1.4__ Size-weighted overall coherence\n",
    "\n",
    "Primary metric for pipeline cluster coherence, weighted to counteract disproportional clusters.\n",
    "\n",
    "$$\n",
    "    \\text{coh}_{weighted} = \\frac{\\sum^{K-1}_{k=0}{n_k \\text{coh}(k)}}{\\sum^{K-1}_{k=0}{n_k}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "__2. Cluster and Noise Fraction__\n",
    "\n",
    "Let the set of clustered points be \n",
    "\n",
    "$$\n",
    "\\mathcal{C} = \\{i | \\ell_i \\neq -1 \\}, \\ \\ |\\mathcal{C}| = N_c\n",
    "$$\n",
    "\n",
    "let the set of noise points be \n",
    "\n",
    "$$\n",
    "\\mathcal{N} = \\{i | \\ell_i = -1\\},\\ \\  |\\mathcal{N}| = N_n\n",
    "$$\n",
    "\n",
    "and $N = N_c + N_n$.\n",
    "\n",
    "Then we define the Noise Fraction as `noise_frac` = $\\frac{N_n}{N}$ and Cluster Fraction as `cluster_frac` = $\\frac{N_c}{N}$\n",
    "\n",
    "\n",
    "__3. Sanity Metrics__\n",
    "\n",
    "Avoids trivial cases\n",
    "1.  Number of clusters: $K$\n",
    "2.  Largest cluster fraction:\n",
    "$$\n",
    "\\frac{max_k n_k}{N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943b833e",
   "metadata": {},
   "source": [
    "### Methodology Part II: Comparative Pipeline Evaluation and Stability Analysis\n",
    "---\n",
    "\n",
    "__1. Cross-Pipeline Evaluation__\n",
    "\n",
    "To determine the superior dimensionality reduction strategy, we compare the optimized PCA and UMAP pipelines. This comparison weighs the trade-off between internal semantic density and topological robustness.\n",
    "\n",
    "- __Relative Coherence Gain__: The percentage difference in $\\text{coh}_{weighted}$ between PCA and UMAP.\n",
    "- __Granularity__: A comparison of $K_{pca}$ versus $K_{umap}$ to identify which method captures a more nuanced topic hierarchy.\n",
    "- __Case-by-case observations__: View select clusters by hand and document observervations.\n",
    "\n",
    "__2. Stability Analysis via Subsampling__\n",
    "\n",
    "To ensure the discovered clusters represent robust semantic structures rather than stochastic one-offs, we perform a stability stress test.\n",
    "\n",
    "__2.1__ Subsampling Procedure\n",
    "\n",
    "We perform $M$ iterations of subsampling without replacement.\n",
    "\n",
    "- In each iteration $m$, a subset $I_m \\subset I$ is drawn such that $|I_m| = 0.8N$.\n",
    "- We then run the optimized PCA and UMAP pipelines for each iteration.\n",
    "\n",
    "*_Subsampling without replacement is critical to prevent \"artificial coherence\" caused by identical vector duplicates, which would otherwise bias the cosine similarity scores._\n",
    "\n",
    "__2.2__ Consistency Metrics\n",
    "\n",
    "For each iteration $m$, we re-calculate and track metrics like _relative coherence gain_, $K$, $\\text{coh}_{weighted}$. After all M iterations we observe:\n",
    "    \n",
    "- __Cluster Count Variance__ ($\\sigma^2_K$): Measures the decisiveness of the pipeline.\n",
    "$$\n",
    "\\sigma^2_K = \\frac{1}{M-1} \\sum_{m=1}^{M} (K_m - \\bar{K})^2\n",
    "$$\n",
    "\n",
    "- __Coherence Stability__: The standard deviation of $\\text{coh}_{weighted}$ across runs, indicating if the \"thematic tightness\" is dependent on specific articles.\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\sigma_{\\text{coh}_{weighted}} = \\sqrt{\\frac{1}{M-1} \\sum_{m=1}^{M} ({\\text{coh}_{weighted}}_m - {\\text{coh}_{weighted}}_{avg})^2}\n",
    "$$\n",
    "\n",
    "- __Adjusted Rand Index (ARI)__: The primary measure of __Label Persistence__. For the set of points present in both the original run (labels $L_{orig}$) and the subsampled run (labels $L_m$), we calculate:\n",
    "\n",
    "$$ARI(L_{orig}, L_m) = \\frac{\\sum_{ij} \\binom{n_{ij}}{2} - [\\sum_i \\binom{a_i}{2} \\sum_j \\binom{b_j}{2}] / \\binom{n}{2}}{\\frac{1}{2} [\\sum_i \\binom{a_i}{2} + \\sum_j \\binom{b_j}{2}] - [\\sum_i \\binom{a_i}{2} \\sum_j \\binom{b_j}{2}] / \\binom{n}{2}}$$\n",
    "- An $ARI \\approx 1.0$ indicates perfect cluster survival.\n",
    "- An $ARI \\approx 0$ indicates that the clusters are essentially random and do not persist across data perturbations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fb8639",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220260e1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
